{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # folder containing the data'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, numpy as np, matplotlib.pyplot as pltcd path/to/your/project   # Navigate to folder\n",
    "git init                  # Initialize git\n",
    "\n",
    "folder = r\"C:/Users/charv/Downloads/fashion-minst/final\"\n",
    "\n",
    "\" # folder containing the data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 class\n",
      "Loaded 1 class\n",
      "Loaded 2 class\n",
      "Loaded 3 class\n",
      "Loaded 4 class\n",
      "Loaded 5 class\n",
      "Loaded 6 class\n",
      "Loaded 7 class\n",
      "Loaded 8 class\n",
      "Loaded 9 class\n",
      "After reshaping\n",
      "(60000, 784) (60000, 10)\n",
      "[0.         0.         0.         0.         0.         0.00392157\n",
      " 0.         0.         0.         0.         0.15686275 0.7372549\n",
      " 0.40392157 0.21176471 0.1882353  0.16862746 0.34117648 0.65882355\n",
      " 0.52156866 0.0627451  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00392157 0.         0.         0.         0.1882353\n",
      " 0.53333336 0.85882354 0.84705883 0.8901961  0.9254902  1.\n",
      " 1.         1.         1.         0.8509804  0.84313726 0.99607843\n",
      " 0.90588236 0.627451   0.17254902 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.6901961  0.87058824 0.8784314  0.827451\n",
      " 0.79607844 0.7764706  0.7647059  0.78431374 0.84313726 0.8\n",
      " 0.7921569  0.7882353  0.7882353  0.7882353  0.81960785 0.85490197\n",
      " 0.8784314  0.6392157  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.7372549\n",
      " 0.85882354 0.78431374 0.7764706  0.7921569  0.7764706  0.78039217\n",
      " 0.78039217 0.7882353  0.7647059  0.7764706  0.7764706  0.78431374\n",
      " 0.78431374 0.78431374 0.78431374 0.7882353  0.78431374 0.88235295\n",
      " 0.15686275 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.2        0.85882354 0.78039217 0.79607844\n",
      " 0.79607844 0.827451   0.93333334 0.972549   0.98039216 0.9607843\n",
      " 0.9764706  0.9647059  0.96862745 0.9882353  0.972549   0.92156863\n",
      " 0.8117647  0.79607844 0.79607844 0.87058824 0.54901963 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.45490196 0.8862745  0.80784315 0.8        0.8117647  0.8\n",
      " 0.39607844 0.29411766 0.18431373 0.28627452 0.1882353  0.19607843\n",
      " 0.17254902 0.2        0.24705882 0.44313726 0.87058824 0.7921569\n",
      " 0.80784315 0.8627451  0.8784314  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.78431374 0.87058824\n",
      " 0.81960785 0.79607844 0.84313726 0.78431374 0.         0.27450982\n",
      " 0.38039216 0.         0.40392157 0.23137255 0.26666668 0.2784314\n",
      " 0.1882353  0.         0.85882354 0.80784315 0.8392157  0.8235294\n",
      " 0.98039216 0.14901961 0.         0.         0.         0.\n",
      " 0.         0.         0.96862745 0.85490197 0.827451   0.8235294\n",
      " 0.84313726 0.8392157  0.         0.99607843 0.9529412  0.54509807\n",
      " 1.         0.68235296 0.9843137  1.         0.8039216  0.\n",
      " 0.84313726 0.8509804  0.8392157  0.8156863  0.8627451  0.37254903\n",
      " 0.         0.         0.         0.         0.         0.17254902\n",
      " 0.8862745  0.8392157  0.8392157  0.84313726 0.8784314  0.8039216\n",
      " 0.         0.16470589 0.13725491 0.23529412 0.0627451  0.06666667\n",
      " 0.04705882 0.05098039 0.27450982 0.         0.7411765  0.84705883\n",
      " 0.827451   0.80784315 0.827451   0.6117647  0.         0.\n",
      " 0.         0.         0.         0.6392157  0.92156863 0.8392157\n",
      " 0.827451   0.8627451  0.84705883 0.7882353  0.20392157 0.2784314\n",
      " 0.34901962 0.36862746 0.3254902  0.30588236 0.27450982 0.29803923\n",
      " 0.36078432 0.34117648 0.80784315 0.8117647  0.87058824 0.8352941\n",
      " 0.85882354 0.8156863  0.         0.         0.         0.\n",
      " 0.         0.4117647  0.73333335 0.8745098  0.92941177 0.972549\n",
      " 0.827451   0.7764706  0.9882353  0.98039216 0.972549   0.9607843\n",
      " 0.972549   0.9882353  0.99215686 0.98039216 0.9882353  0.9372549\n",
      " 0.7882353  0.827451   0.88235295 0.84313726 0.75686276 0.44313726\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.06666667 0.21176471 0.62352943 0.87058824 0.75686276\n",
      " 0.8156863  0.7529412  0.77254903 0.78431374 0.78431374 0.78431374\n",
      " 0.78431374 0.7882353  0.79607844 0.7647059  0.8235294  0.64705884\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.18431373 0.88235295 0.7529412  0.8392157  0.79607844\n",
      " 0.80784315 0.8        0.8        0.8039216  0.80784315 0.8\n",
      " 0.827451   0.77254903 0.85490197 0.41960785 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.00392157 0.02352941 0.         0.18039216\n",
      " 0.827451   0.7647059  0.827451   0.7921569  0.80784315 0.8039216\n",
      " 0.8        0.8039216  0.80784315 0.8        0.827451   0.78431374\n",
      " 0.85490197 0.35686275 0.         0.01176471 0.00392157 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00392157 0.         0.04313726 0.77254903 0.78039217\n",
      " 0.8039216  0.7921569  0.8039216  0.80784315 0.8        0.8039216\n",
      " 0.8117647  0.8        0.8039216  0.8039216  0.85490197 0.3019608\n",
      " 0.         0.01960784 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.01176471\n",
      " 0.         0.00784314 0.7490196  0.7764706  0.7882353  0.8039216\n",
      " 0.80784315 0.8039216  0.8039216  0.80784315 0.81960785 0.80784315\n",
      " 0.78039217 0.81960785 0.85882354 0.28627452 0.         0.01960784\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.00784314 0.         0.\n",
      " 0.7372549  0.77254903 0.78431374 0.8117647  0.8117647  0.8\n",
      " 0.8117647  0.8117647  0.8235294  0.8156863  0.7764706  0.8117647\n",
      " 0.8666667  0.28235295 0.         0.01568628 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00784314 0.         0.         0.84313726 0.7764706\n",
      " 0.79607844 0.80784315 0.8156863  0.8039216  0.8117647  0.8117647\n",
      " 0.8235294  0.8156863  0.78431374 0.7921569  0.87058824 0.29411766\n",
      " 0.         0.01568628 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00392157\n",
      " 0.         0.         0.827451   0.7764706  0.81960785 0.80784315\n",
      " 0.81960785 0.80784315 0.8156863  0.8117647  0.827451   0.80784315\n",
      " 0.8039216  0.7764706  0.8666667  0.3137255  0.         0.01176471\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.00392157 0.         0.\n",
      " 0.8        0.7882353  0.8039216  0.8156863  0.8117647  0.8039216\n",
      " 0.827451   0.8039216  0.8235294  0.8235294  0.81960785 0.7647059\n",
      " 0.8666667  0.3764706  0.         0.01176471 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00392157 0.         0.         0.7921569  0.7882353\n",
      " 0.8039216  0.81960785 0.8117647  0.8039216  0.8352941  0.80784315\n",
      " 0.8235294  0.81960785 0.8235294  0.7607843  0.8509804  0.4117647\n",
      " 0.         0.00784314 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00392157\n",
      " 0.         0.         0.8        0.8        0.8039216  0.8156863\n",
      " 0.8117647  0.8039216  0.84313726 0.8117647  0.8235294  0.8156863\n",
      " 0.827451   0.75686276 0.8352941  0.4509804  0.         0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.8        0.8117647  0.8117647  0.8156863  0.80784315 0.80784315\n",
      " 0.84313726 0.8235294  0.8235294  0.8117647  0.827451   0.7647059\n",
      " 0.8235294  0.4627451  0.         0.00784314 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00392157 0.         0.         0.7764706  0.8156863\n",
      " 0.8156863  0.8156863  0.8        0.8117647  0.827451   0.827451\n",
      " 0.8235294  0.8117647  0.827451   0.7647059  0.8117647  0.4745098\n",
      " 0.         0.00392157 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00392157\n",
      " 0.         0.         0.7764706  0.8235294  0.8117647  0.8156863\n",
      " 0.80784315 0.81960785 0.8352941  0.827451   0.827451   0.8117647\n",
      " 0.8235294  0.77254903 0.8117647  0.4862745  0.         0.00392157\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.6745098  0.8235294  0.79607844 0.7882353  0.78039217 0.8\n",
      " 0.8117647  0.8039216  0.8        0.7882353  0.8039216  0.77254903\n",
      " 0.80784315 0.49803922 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.7372549  0.8666667\n",
      " 0.8392157  0.91764706 0.9254902  0.93333334 0.9529412  0.9529412\n",
      " 0.9529412  0.9411765  0.9529412  0.8392157  0.8784314  0.63529414\n",
      " 0.         0.00784314 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00392157\n",
      " 0.         0.         0.54509807 0.57254905 0.50980395 0.5294118\n",
      " 0.5294118  0.5372549  0.49019608 0.4862745  0.49019608 0.4745098\n",
      " 0.46666667 0.44313726 0.50980395 0.29803923 0.         0.\n",
      " 0.         0.         0.         0.        ] [1 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Now we try to load the images and their corresponding labels into memory\n",
    "\n",
    "def load_data(X, y):\n",
    "    for f in os.listdir(folder):\n",
    "        for file in os.listdir(f\"{folder}/{f}\"):\n",
    "            img = plt.imread(f\"{folder}/{f}/{file}\")\n",
    "            X.append(img)\n",
    "\n",
    "            # The most obvious choice for the label is the class (folder) name\n",
    "            # label = int(f)\n",
    "\n",
    "            # [Q1] But we dont use it here why? Why is it an array of 10 elements?\n",
    "            # Clue: Lookup one hot encoding\n",
    "            # Read up on Cross Entropy Loss\n",
    "\n",
    "            label = [0] * 10\n",
    "            label[int(f)] = 1 # Why is this array the label and not a numeber?\n",
    "\n",
    "            y.append(label)\n",
    "            \n",
    "        print(f\"Loaded {f} class\")\n",
    "\n",
    "X, y = [], []\n",
    "load_data(X, y)\n",
    "\n",
    "# [Q2] Why convert to numpy array?\n",
    "\"\"\"\n",
    ".\n",
    "convert x and y to numpy arrays here\n",
    ".\n",
    "\"\"\"\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X = X[:, :,:, 0] # [Q3] Why are we doing this and what does this type of slicing result in?\n",
    "X = X.reshape(X.shape[0], X.shape[1]*X.shape[2]) # [Q4] Why are we reshaping the data?\n",
    "print(\"After reshaping\")\n",
    "print(X.shape, y.shape)\n",
    "print(X[0], y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, input_neurons, hidden_neurons, output_neurons, learning_rate, epochs):\n",
    "        \"\"\"\n",
    "        Class Definition\n",
    "        \n",
    "        We use a class because it is easy to visualize the process of training a neural network\n",
    "        It's also easier to resuse and repurpose depending on the task at hand\n",
    "\n",
    "        We have a simple neural network, with an input layer, one hidden (middle) layer and an output layer\n",
    "\n",
    "        input_neurons: Number of neurons in the input layer\n",
    "        hidden_neurons: Number of neurons in the hidden layer\n",
    "        output_neurons: Number of neurons in the output layer\n",
    "        learning_rate: The rate at which the weights are updated [Q5] What is the learning rate?\n",
    "        epochs: Number of times the model will train on the entire dataset \n",
    "        \"\"\"\n",
    "\n",
    "        self.input_neurons = input_neurons\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.output_neurons = output_neurons\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        \"\"\"\n",
    "        Weights and Biases\n",
    "\n",
    "        At this point you should know what weights and biases are in a neural network and if not, go check out the 3blue1brown video on Neural Networks\n",
    "        What matters here is however the matrix dimensions of the weights and biases\n",
    "\n",
    "        [Q6] Why are the dimensions of the weights and biases the way they are?\n",
    "        \n",
    "        Try to figure out the dimensions of the weights and biases for the hidden and output layers\n",
    "        Try to see what equations represent the forward pass (basically the prediction)\n",
    "        And then, try to see if the dimensions of the matrix multiplications are correct\n",
    "\n",
    "        Note: The bias dimensions may not match. Look up broadcasting in numpy to understand\n",
    "        [Q7] What is broadcasting and why do we need to broadcast the bias?\n",
    "        \"\"\"\n",
    "\n",
    "        # Ideally any random set of weights and biases can be used to initialize the network\n",
    "        # self.wih = np.random.randn(hidden_neurons, input_neurons)\n",
    "\n",
    "        # [Q8] What is np.random.randn? What's the shape of this matrix?\n",
    "\n",
    "        # Optional: Try to figure out why the weights are initialized this way\n",
    "        # Note: You can just use the commented out line above if you don't want to do this\n",
    "\n",
    "        self.wih = np.random.randn(hidden_neurons, input_neurons) * np.sqrt(2/input_neurons)\n",
    "        self.bih = np.zeros((hidden_neurons, 1))\n",
    "\n",
    "        self.who = np.random.randn(output_neurons, hidden_neurons) * np.sqrt(2/hidden_neurons)\n",
    "        self.bho = np.zeros((output_neurons, 1))\n",
    "\n",
    "    # Activation Functions and their derivatives\n",
    "    # [Q9] What are activation functions and why do we need them?\n",
    "\n",
    "    def relu(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the RELU function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return z * (z > 0)\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the Sigmoid function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def relu_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the RELU derivative function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return 1 * (z > 0)\n",
    "\n",
    "    def sigmoid_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the Sigmoid derivative function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return z * (1 - z)\n",
    "\n",
    "    # [Q10] What is the softmax function and why do we need it? Read up on it\n",
    "    def softmax(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the Softmax function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "\n",
    "    def softmax_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the Softmax derivative function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return z * (1 - z)\n",
    "\n",
    "    # Loss Functions and their derivatives\n",
    "    # [Q11] What are loss functions and why do we need them?\n",
    "\n",
    "    def mean_squared_error(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Implementation of the Mean Squared Error function\n",
    "        y: (10, n)\n",
    "        y_hat: (10, n)\n",
    "        returns (1, n)\n",
    "        \"\"\"\n",
    "        return np.mean((y - y_hat) ** 2, axis=0)\n",
    "\n",
    "    def cross_entropy_loss(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Implementation of the Cross Entropy Loss function\n",
    "        y: (10, n)\n",
    "        y_hat: (10, n)\n",
    "        returns (1, n)\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        # Implement the cross entropy loss function here and return it\n",
    "        # Keep the dimensions of the input in mind when writing the code\n",
    "\n",
    "        # [Code Goes Here]\n",
    "        loss = -np.sum(y * np.log(y_hat), axis=0, keepdims=True)  # shape (1, n)\n",
    "        return loss\n",
    "\n",
    "\n",
    "        pass\n",
    "\n",
    "    def mean_squared_error_derivative(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Implementation of the Mean Squared Error derivative function\n",
    "        y: (10, n)\n",
    "        y_hat: (10, n)\n",
    "        returns (10, n)\n",
    "        \"\"\"\n",
    "        return y_hat - y\n",
    "\n",
    "    def cross_entropy_derivative(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Implementation of the Cross Entropy Loss derivative function\n",
    "        y: (10, n)\n",
    "        y_hat: (10, n)\n",
    "        returns (10, n)\n",
    "        \"\"\"\n",
    "\n",
    "        # Implement the cross entropy loss derivative function here and return it\n",
    "        # Note: The derivative of the CEL is usually taken with respect to the softmax input not output so keep that in mind while writing\n",
    "\n",
    "        # [Code Goes Here]\n",
    "        return y_hat - y\n",
    "        pass\n",
    "\n",
    "    # Forward propagation\n",
    "    def forward(self, input_list):\n",
    "        \"\"\"\n",
    "        Implementation of the Forward Pass\n",
    "        input_list: (784, n)        - n is the number of images\n",
    "        returns (10, n)              - n is the number of images\n",
    "    \n",
    "        Now we come to the heart of the neural network, the forward pass\n",
    "        This is where the input is passed through the network to get the output\n",
    "\n",
    "        [Q12] What does the output choice we have here mean? It's an array of 10 elements per image, but why?\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = np.array(input_list, ndmin=2).T\n",
    "        inputs = inputs - np.mean(inputs) # [Q13] Why are we subtracting the mean of the inputs?\n",
    "        \n",
    "        # To get to the hidden layer:\n",
    "        # Multiply the input with the weights and adding the bias\n",
    "        # Apply the activation function (relu in this case)\n",
    "\n",
    "        # \n",
    "        \n",
    "\n",
    "        # To get to the output layer:\n",
    "        # Multiply the hidden layer output with the weights and adding the bias\n",
    "        # Apply the activation function (softmax in this case)\n",
    "        # [Q14] Why are we using the softmax function here?\n",
    "\n",
    "        # [Code Goes Here]\n",
    "        z1 = np.dot(self.wih, inputs) + self.bih\n",
    "        a1 = self.relu(z1)  \n",
    "        z2 = np.dot(self.who, a1) + self.bho \n",
    "        y_hat = self.softmax(z2) \n",
    "         \n",
    "\n",
    "\n",
    "        # Return it\n",
    "\n",
    "        # [Code Goes Here]\n",
    "        return y_hat\n",
    "\n",
    "    # Back propagation\n",
    "    def backprop(self, inputs_list, targets_list):\n",
    "        \"\"\"\n",
    "        Implementation of the Backward Pass\n",
    "        inputs_list: (784, n)\n",
    "        targets_list: (10, n)\n",
    "        returns a scalar value (loss)\n",
    "        \n",
    "        This is where the magic happens, the backpropagation algorithm\n",
    "        This is where the weights are updated based on the error in the prediction of the network\n",
    "\n",
    "        Now, the calculus involved is fairly complicated, especially because it's being done in matrix form\n",
    "        However the intuition is simple. \n",
    "\n",
    "        Since this is a recruitment stage, most of the function is written out for you, so follow along with the comments\n",
    "        \"\"\"\n",
    "\n",
    "        # Basic forward pass to get the outputs\n",
    "        # Obviously we need the predictions to know how the model is doing\n",
    "        # [Q15] Why are we doing a forward pass here instead of just using the outputs from the forward function?\n",
    "        # Is there any actual reason, or could we just swap it?\n",
    "\n",
    "        inputs = np.array(inputs_list, ndmin=2).T # (784, n)\n",
    "        inputs = inputs - np.mean(inputs)\n",
    "\n",
    "        tj = np.array(targets_list, ndmin=2).T # (10, n)\n",
    "\n",
    "        hidden_inputs = np.dot(self.wih, inputs) + self.bih\n",
    "        hidden_outputs = self.relu(hidden_inputs)\n",
    "\n",
    "        final_inputs = np.dot(self.who, hidden_outputs) + self.bho\n",
    "        yj = self.softmax(final_inputs)\n",
    "\n",
    "        # Calculating the loss - This is the error in the prediction\n",
    "        # The loss then is the indication of how well the model is doing, its a useful parameter to track to see if the model is improving\n",
    "\n",
    "        loss = self.mean_squared_error(tj, yj) # Convert this to cross entropy loss\n",
    "\n",
    "\n",
    "        # Updating the weights using Update Rule\n",
    "        # Now that we have the incorrect predictions, we can update the weights to make the predictions better\n",
    "        # This is done using the gradient of the loss function with respect to the weights\n",
    "        # Basically, we know how much the overall error is caused due to individual weights using the chain rule of calculus\n",
    "        # Since we want to minimise the error, we move in the opposite direction of something like a \"derivative\" of the error with respect to the weights\n",
    "        # Calculus therefore helps us find the direction in which we should move to reduce the error\n",
    "        # A direction means what delta W changes we need to make to make the model better\n",
    "\n",
    "        # Output Layer - We start with the output layer because we are backtracking how the error is caused\n",
    "        # Think of it as using the derivatives of each layer while going back\n",
    "\n",
    "\n",
    "        # For the task, you will be using Cross Entropy Loss\n",
    "\n",
    "        # Change this to cross entropy loss\n",
    "        dE_dzo = yj - tj\n",
    "        # (10,n)\n",
    "        # Note: the derivative of the CEL is usually taken with respect to the softmax input not output so keep that in mind while writing\n",
    "\n",
    "\n",
    "        dE_dwho = np.dot(dE_dzo, hidden_outputs.T) / hidden_outputs.shape[1] # dot((10,n) (n,128) = (10,128)\n",
    "        dE_dbho = np.mean(dE_dzo, axis=1, keepdims=True) # sum((10,n), axis=1) = (10,1)\n",
    "        \n",
    "        self.who -= self.lr * dE_dwho\n",
    "        self.bho -= self.lr * dE_dbho\n",
    "\n",
    "        # Hidden Layer\n",
    "        dE_dah = np.dot(self.who.T, dE_dzo) # dot((128,10), (10,n)) = (128,n)\n",
    "        dE_dzh = dE_dah * self.relu_derivative(hidden_inputs)\n",
    "        dE_dwih = np.dot(dE_dzh, inputs.T) / inputs.shape[1]\n",
    "        dE_dbih = np.mean(dE_dzh, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "        self.wih -= self.lr * dE_dwih\n",
    "        self.bih -= self.lr * dE_dbih\n",
    "\n",
    "        return np.mean(loss)\n",
    "    \n",
    "    def fit(self, inputs_list, targets_list,validation_data, validation_labels):\n",
    "        \"\"\"\n",
    "        Implementation of the training loop\n",
    "        inputs_list: (784, n)\n",
    "        targets_list: (10, n)\n",
    "        validation_data: (784, n)\n",
    "        validation_labels: (10, n)\n",
    "        returns train_loss, val_loss\n",
    "\n",
    "        This is where the training loop is implemented\n",
    "        We loop over the entire dataset for a certain number of epochs\n",
    "        We also track the validation loss to see how well the model is generalizing\n",
    "        [Q16] What is the validation dataset and what do we mean by generalization?\n",
    "\n",
    "        We also return the training and validation loss to see how the model is improving\n",
    "        It's a good idea to plot these to see how the model is doing\n",
    "        \"\"\"\n",
    "\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "        for epoch in range(self.epochs):\n",
    "            loss = self.backprop(inputs_list, targets_list)\n",
    "            train_loss.append(loss)\n",
    "            vloss = self.mean_squared_error(validation_labels.T, self.forward(validation_data))\n",
    "            val_loss.append(np.mean(vloss)) \n",
    "            print(f\"Epoch: {epoch}, Loss: {loss}, Val Loss: {val_loss[-1]}\")\n",
    "\n",
    "        return train_loss[1:], val_loss[:-1] \n",
    "\n",
    "    def predict(self, X):\n",
    "        outputs = self.forward(X).T\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the entire NN class definition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.09382831955323807, Val Loss: 0.09164297650230215\n",
      "Epoch: 1, Loss: 0.09171227170347075, Val Loss: 0.08976944388415244\n",
      "Epoch: 2, Loss: 0.08985835561921275, Val Loss: 0.08806669009165914\n",
      "Epoch: 3, Loss: 0.08817291906773932, Val Loss: 0.08647921157990494\n",
      "Epoch: 4, Loss: 0.0866012263738362, Val Loss: 0.08497485515329793\n",
      "Epoch: 5, Loss: 0.08511106311511663, Val Loss: 0.08353480220383434\n",
      "Epoch: 6, Loss: 0.08368445725835434, Val Loss: 0.0821481059841443\n",
      "Epoch: 7, Loss: 0.08231004775747192, Val Loss: 0.08080723154580251\n",
      "Epoch: 8, Loss: 0.08098061578663236, Val Loss: 0.07950595717829818\n",
      "Epoch: 9, Loss: 0.07969167098026952, Val Loss: 0.07824102737878828\n",
      "Epoch: 10, Loss: 0.07843905381314235, Val Loss: 0.07700861462223063\n",
      "Epoch: 11, Loss: 0.07721851256461261, Val Loss: 0.07580754691570066\n",
      "Epoch: 12, Loss: 0.07602859486539053, Val Loss: 0.07463562519584116\n",
      "Epoch: 13, Loss: 0.07486724915984772, Val Loss: 0.07349159402978042\n",
      "Epoch: 14, Loss: 0.07373311988009675, Val Loss: 0.07237467530952488\n",
      "Epoch: 15, Loss: 0.07262587654034255, Val Loss: 0.07128443142456037\n",
      "Epoch: 16, Loss: 0.07154479698985249, Val Loss: 0.07022083438379716\n",
      "Epoch: 17, Loss: 0.07048979562032108, Val Loss: 0.06918411193375831\n",
      "Epoch: 18, Loss: 0.06946070612241272, Val Loss: 0.06817427746419996\n",
      "Epoch: 19, Loss: 0.06845785853163953, Val Loss: 0.06719233030970809\n",
      "Epoch: 20, Loss: 0.06748243060863507, Val Loss: 0.06623826436331993\n",
      "Epoch: 21, Loss: 0.06653429210394794, Val Loss: 0.06531244849386751\n",
      "Epoch: 22, Loss: 0.06561333397215072, Val Loss: 0.06441468113277618\n",
      "Epoch: 23, Loss: 0.06471939632337025, Val Loss: 0.06354529371306492\n",
      "Epoch: 24, Loss: 0.06385314192726863, Val Loss: 0.06270418797725301\n",
      "Epoch: 25, Loss: 0.0630149793254304, Val Loss: 0.06189157242969907\n",
      "Epoch: 26, Loss: 0.06220497790884605, Val Loss: 0.061106918295081074\n",
      "Epoch: 27, Loss: 0.0614226703050689, Val Loss: 0.06035015063208967\n",
      "Epoch: 28, Loss: 0.06066805158835658, Val Loss: 0.05962103133374719\n",
      "Epoch: 29, Loss: 0.05994021684844955, Val Loss: 0.058918448102190005\n",
      "Epoch: 30, Loss: 0.0592384896018964, Val Loss: 0.058241769951554404\n",
      "Epoch: 31, Loss: 0.05856238764584506, Val Loss: 0.057589934685917624\n",
      "Epoch: 32, Loss: 0.05791089327034154, Val Loss: 0.05696194584729966\n",
      "Epoch: 33, Loss: 0.057283247895100996, Val Loss: 0.05635715737622838\n",
      "Epoch: 34, Loss: 0.05667855600226987, Val Loss: 0.05577464393439521\n",
      "Epoch: 35, Loss: 0.056095830367659215, Val Loss: 0.0552135831345527\n",
      "Epoch: 36, Loss: 0.05553435327702231, Val Loss: 0.054673116445500075\n",
      "Epoch: 37, Loss: 0.05499304687976148, Val Loss: 0.05415228263798618\n",
      "Epoch: 38, Loss: 0.0544710830383939, Val Loss: 0.053650132884738314\n",
      "Epoch: 39, Loss: 0.05396755269640525, Val Loss: 0.053165870265691445\n",
      "Epoch: 40, Loss: 0.05348165680981425, Val Loss: 0.05269893818418871\n",
      "Epoch: 41, Loss: 0.053012830079831354, Val Loss: 0.05224816188261304\n",
      "Epoch: 42, Loss: 0.05256017605972017, Val Loss: 0.051813186485916166\n",
      "Epoch: 43, Loss: 0.05212311359404882, Val Loss: 0.0513933505332014\n",
      "Epoch: 44, Loss: 0.05170084468991404, Val Loss: 0.0509878616424495\n",
      "Epoch: 45, Loss: 0.051292734705390905, Val Loss: 0.05059610787694514\n",
      "Epoch: 46, Loss: 0.050898147520321256, Val Loss: 0.0502174218603665\n",
      "Epoch: 47, Loss: 0.05051641129474178, Val Loss: 0.049851240825196194\n",
      "Epoch: 48, Loss: 0.050146953729721645, Val Loss: 0.0494969626619504\n",
      "Epoch: 49, Loss: 0.049789344329557804, Val Loss: 0.04915399285375531\n",
      "Epoch: 50, Loss: 0.04944303705097426, Val Loss: 0.04882190699374591\n",
      "Epoch: 51, Loss: 0.04910749889553154, Val Loss: 0.04850029178900519\n",
      "Epoch: 52, Loss: 0.04878236961169934, Val Loss: 0.04818862622269185\n",
      "Epoch: 53, Loss: 0.04846715083002792, Val Loss: 0.04788666238363995\n",
      "Epoch: 54, Loss: 0.04816148589888005, Val Loss: 0.04759382679378777\n",
      "Epoch: 55, Loss: 0.047864894590228635, Val Loss: 0.047309686131238454\n",
      "Epoch: 56, Loss: 0.04757692005304872, Val Loss: 0.04703401914189633\n",
      "Epoch: 57, Loss: 0.04729735181945544, Val Loss: 0.04676650409887336\n",
      "Epoch: 58, Loss: 0.047025720683785854, Val Loss: 0.046506635730956605\n",
      "Epoch: 59, Loss: 0.046761687734216795, Val Loss: 0.046254204102128514\n",
      "Epoch: 60, Loss: 0.046505028071924956, Val Loss: 0.04600880854731099\n",
      "Epoch: 61, Loss: 0.046255294549252375, Val Loss: 0.04577016998672201\n",
      "Epoch: 62, Loss: 0.046012286230069005, Val Loss: 0.04553793645664451\n",
      "Epoch: 63, Loss: 0.04577569395233345, Val Loss: 0.04531188778740632\n",
      "Epoch: 64, Loss: 0.045545228119923384, Val Loss: 0.04509178574098793\n",
      "Epoch: 65, Loss: 0.045320700139458873, Val Loss: 0.044877364058786555\n",
      "Epoch: 66, Loss: 0.045101835835335176, Val Loss: 0.04466838414003229\n",
      "Epoch: 67, Loss: 0.044888476835889625, Val Loss: 0.0444646468523027\n",
      "Epoch: 68, Loss: 0.04468040605405135, Val Loss: 0.044265935769404845\n",
      "Epoch: 69, Loss: 0.04447737987738934, Val Loss: 0.044072029874090694\n",
      "Epoch: 70, Loss: 0.044279174314865256, Val Loss: 0.043882754561889704\n",
      "Epoch: 71, Loss: 0.04408560529446639, Val Loss: 0.04369792258438226\n",
      "Epoch: 72, Loss: 0.043896527528620236, Val Loss: 0.043517341026349396\n",
      "Epoch: 73, Loss: 0.043711724218146696, Val Loss: 0.04334084909150412\n",
      "Epoch: 74, Loss: 0.043531017931450436, Val Loss: 0.04316836122382708\n",
      "Epoch: 75, Loss: 0.04335432080076117, Val Loss: 0.04299973716370903\n",
      "Epoch: 76, Loss: 0.043181557101746475, Val Loss: 0.042834789916102387\n",
      "Epoch: 77, Loss: 0.04301254137916473, Val Loss: 0.04267343611317531\n",
      "Epoch: 78, Loss: 0.04284714202419499, Val Loss: 0.04251553439604698\n",
      "Epoch: 79, Loss: 0.042685227964899056, Val Loss: 0.0423609700070291\n",
      "Epoch: 80, Loss: 0.04252665960867128, Val Loss: 0.04220959220726818\n",
      "Epoch: 81, Loss: 0.04237134301469565, Val Loss: 0.0420613052851656\n",
      "Epoch: 82, Loss: 0.042219202927170685, Val Loss: 0.041916032210069884\n",
      "Epoch: 83, Loss: 0.04207010924816227, Val Loss: 0.04177360440001926\n",
      "Epoch: 84, Loss: 0.04192387535804088, Val Loss: 0.04163392053461723\n",
      "Epoch: 85, Loss: 0.04178041580438613, Val Loss: 0.04149695422246613\n",
      "Epoch: 86, Loss: 0.04163969335990968, Val Loss: 0.04136262067742033\n",
      "Epoch: 87, Loss: 0.04150157869934291, Val Loss: 0.04123087621027118\n",
      "Epoch: 88, Loss: 0.04136605855508935, Val Loss: 0.041101588791943346\n",
      "Epoch: 89, Loss: 0.04123297904007779, Val Loss: 0.040974698145935844\n",
      "Epoch: 90, Loss: 0.04110225725019733, Val Loss: 0.04085012158558755\n",
      "Epoch: 91, Loss: 0.04097385340093716, Val Loss: 0.040727741520630065\n",
      "Epoch: 92, Loss: 0.040847665885596116, Val Loss: 0.040607513906463014\n",
      "Epoch: 93, Loss: 0.04072365598468894, Val Loss: 0.04048936985639373\n",
      "Epoch: 94, Loss: 0.04060174800050655, Val Loss: 0.04037320868224848\n",
      "Epoch: 95, Loss: 0.0404818838838519, Val Loss: 0.040258960792639895\n",
      "Epoch: 96, Loss: 0.04036401718362494, Val Loss: 0.04014662032082233\n",
      "Epoch: 97, Loss: 0.04024807892198713, Val Loss: 0.040036106129117746\n",
      "Epoch: 98, Loss: 0.040134005134014335, Val Loss: 0.03992736815415174\n",
      "Epoch: 99, Loss: 0.040021759525225485, Val Loss: 0.03982039914982942\n",
      "Epoch: 100, Loss: 0.03991130086536238, Val Loss: 0.03971512996516923\n",
      "Epoch: 101, Loss: 0.03980258289361808, Val Loss: 0.03961153951625421\n",
      "Epoch: 102, Loss: 0.03969556130634867, Val Loss: 0.039509550400852314\n",
      "Epoch: 103, Loss: 0.039590173240351086, Val Loss: 0.03940916802151534\n",
      "Epoch: 104, Loss: 0.03948638808713397, Val Loss: 0.03931029500531211\n",
      "Epoch: 105, Loss: 0.03938412747283241, Val Loss: 0.03921288036106518\n",
      "Epoch: 106, Loss: 0.039283332214621984, Val Loss: 0.039116940028374826\n",
      "Epoch: 107, Loss: 0.039184008822565036, Val Loss: 0.03902240988311147\n",
      "Epoch: 108, Loss: 0.03908612393259757, Val Loss: 0.03892925609586091\n",
      "Epoch: 109, Loss: 0.03898962285404258, Val Loss: 0.03883742125223814\n",
      "Epoch: 110, Loss: 0.03889445168252038, Val Loss: 0.03874685650178302\n",
      "Epoch: 111, Loss: 0.038800539029157435, Val Loss: 0.03865755450397282\n",
      "Epoch: 112, Loss: 0.0387079017131135, Val Loss: 0.038569463432978894\n",
      "Epoch: 113, Loss: 0.03861648255097393, Val Loss: 0.038482551678226874\n",
      "Epoch: 114, Loss: 0.038526269806479975, Val Loss: 0.03839678991314439\n",
      "Epoch: 115, Loss: 0.03843721900105002, Val Loss: 0.03831217672651754\n",
      "Epoch: 116, Loss: 0.038349324972349226, Val Loss: 0.03822868748900049\n",
      "Epoch: 117, Loss: 0.038262603014754724, Val Loss: 0.038146299828528414\n",
      "Epoch: 118, Loss: 0.03817702973537848, Val Loss: 0.03806496549108601\n",
      "Epoch: 119, Loss: 0.03809257141983518, Val Loss: 0.037984667856991144\n",
      "Epoch: 120, Loss: 0.03800917037169773, Val Loss: 0.03790537095252413\n",
      "Epoch: 121, Loss: 0.03792680420413362, Val Loss: 0.03782707311857573\n",
      "Epoch: 122, Loss: 0.03784546265849875, Val Loss: 0.03774976490516868\n",
      "Epoch: 123, Loss: 0.03776509939208458, Val Loss: 0.03767339853025208\n",
      "Epoch: 124, Loss: 0.03768569912533267, Val Loss: 0.03759795443818599\n",
      "Epoch: 125, Loss: 0.037607230366389306, Val Loss: 0.0375234103551952\n",
      "Epoch: 126, Loss: 0.037529672466609314, Val Loss: 0.037449753377212854\n",
      "Epoch: 127, Loss: 0.03745301606613846, Val Loss: 0.03737698291111192\n",
      "Epoch: 128, Loss: 0.037377261589059456, Val Loss: 0.03730509101019189\n",
      "Epoch: 129, Loss: 0.037302391031463156, Val Loss: 0.03723401947405293\n",
      "Epoch: 130, Loss: 0.03722835886648258, Val Loss: 0.03716374708990985\n",
      "Epoch: 131, Loss: 0.03715514964043678, Val Loss: 0.037094265412970596\n",
      "Epoch: 132, Loss: 0.037082749778350015, Val Loss: 0.037025564171909865\n",
      "Epoch: 133, Loss: 0.03701115293728243, Val Loss: 0.03695762110877046\n",
      "Epoch: 134, Loss: 0.03694035041268886, Val Loss: 0.036890437690442146\n",
      "Epoch: 135, Loss: 0.036870347765551495, Val Loss: 0.0368239856418756\n",
      "Epoch: 136, Loss: 0.0368011007428355, Val Loss: 0.036758256586554273\n",
      "Epoch: 137, Loss: 0.03673257884179521, Val Loss: 0.03669321964025821\n",
      "Epoch: 138, Loss: 0.0366647705347701, Val Loss: 0.03662888970721541\n",
      "Epoch: 139, Loss: 0.03659768125061722, Val Loss: 0.03656522896168262\n",
      "Epoch: 140, Loss: 0.03653127261868579, Val Loss: 0.03650221931843713\n",
      "Epoch: 141, Loss: 0.036465519395738284, Val Loss: 0.03643986016672893\n",
      "Epoch: 142, Loss: 0.03640043355106702, Val Loss: 0.03637812613029407\n",
      "Epoch: 143, Loss: 0.0363360121473539, Val Loss: 0.03631700601639689\n",
      "Epoch: 144, Loss: 0.03627222059798377, Val Loss: 0.03625650687836134\n",
      "Epoch: 145, Loss: 0.03620905323615243, Val Loss: 0.0361965991027256\n",
      "Epoch: 146, Loss: 0.03614650576233775, Val Loss: 0.036137287517736365\n",
      "Epoch: 147, Loss: 0.03608456697558314, Val Loss: 0.03607853811511465\n",
      "Epoch: 148, Loss: 0.03602318406830882, Val Loss: 0.03602035391161401\n",
      "Epoch: 149, Loss: 0.03596238075134617, Val Loss: 0.03596275514027109\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7418333333333333\n"
     ]
    }
   ],
   "source": [
    "# This is where the class is used to train the model\n",
    "\n",
    "# The parameters in the model are (input_neurons, hidden_neurons, output_neurons, learning_rate, epochs)\n",
    "# These parameters aren't the right parameters, so tweak them to get the best results\n",
    "# Around 70% accuracy is a good end goal (75% is great) but for the recruitment task, 60% is good enough\n",
    "\n",
    "# [Q17] What are the parameters in the model and what do they mean?\n",
    "\n",
    "fashion_mnist = NN(784, 160, 10, 0.03, 150)\n",
    "p = np.random.permutation(len(X))\n",
    "X, y = X[p], y[p]\n",
    "\n",
    "# Splitting the data into training, validation and testing in the ratio 70:20:10\n",
    "X_train, y_train = X[:int(0.7*len(X))], y[:int(0.7*len(X))]\n",
    "X_val, y_val = X[int(0.7*len(X)):int(0.9*len(X))], y[int(0.7*len(X)):int(0.9*len(X))]\n",
    "X_test, y_test = X[int(0.9*len(X)):], y[int(0.9*len(X)):]\n",
    "\n",
    "# Training the model\n",
    "train_loss,val_loss = fashion_mnist.fit(X_train, y_train,X_val,y_val)\n",
    "\n",
    "\n",
    "# Plotting the loss\n",
    "plt.plot(train_loss,label='train')\n",
    "plt.plot(val_loss,label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_pred = fashion_mnist.predict(X_test)\n",
    "\n",
    "# [Q18] Why are we using argmax here? Why is this output different from the output of the model?\n",
    "y_pred = np.argmax(y_pred, axis=1)  \n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "print(f\"Accuracy: {np.mean(y_pred == y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
